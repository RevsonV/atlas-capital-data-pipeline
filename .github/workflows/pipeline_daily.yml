#.github/workflows/pipeline_daily.yml

name: Daily ELT Pipeline Run (Atlas Capital)

on:
  schedule:
    # Executa diariamente ao meio-dia UTC. Ajuste para um horário de sua preferência.
    - cron: '0 12 * * *' 
  workflow_dispatch: # Permite que você o rode manualmente para testes

jobs:
  run_pipeline:
    runs-on: ubuntu-latest
    
    # Substitua 'atlas_capital_data_pipeline' pelo nome exato da sua pasta de projeto dbt
    env:
      DBT_PROJECT_DIR: ./atlas_capital_data_pipeline 

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'

      - name: Install Python and dbt Dependencies
        run: |
          python -m pip install --upgrade pip
          # Instala as bibliotecas de Python + dbt-bigquery e seus utilitários
          pip install pandas pandas-gbq requests python-dotenv
          pip install dbt-bigquery
          dbt deps # Instala o dbt-utils

      - name: Authenticate GCP/BigQuery (Key Injection)
        # Decodifica a chave Base64 (seu segredo) e a salva temporariamente.
        run: |
          echo "${{ secrets.GCP_SA_KEY_BASE64 }}" | base64 --decode > gcp_sa_key.json
          # Define a variável de ambiente GOOGLE_APPLICATION_CREDENTIALS, necessária para autenticação
          echo "GOOGLE_APPLICATION_CREDENTIALS=gcp_sa_key.json" >> $GITHUB_ENV

      # NOVO PASSO: CRIAÇÃO DO PROFILES.YML DINÂMICO
      - name: Create dbt profiles.yml
        run: |
          # O comando 'cat << EOF > profiles.yml' cria o arquivo profiles.yml na raiz do workspace
          cat << EOF > profiles.yml
          atlas_capital_data_pipeline:
            target: dev
            outputs:
              dev:
                type: bigquery
                method: service-account-json
                project: ${{ secrets.GCP_PROJECT_ID }}
                dataset: stg_atlas_capital # Seu Target Schema de desenvolvimento
                threads: 4
                location: US
                keyfile: gcp_sa_key.json # O NOME DO ARQUIVO SALVO NO PASSO ANTERIOR
          EOF
          
      # PASSO 1: EXECUÇÃO DO PYTHON (E & L)
      - name: Run Python Ingestion Script (E & L)
        run: |
          python src/ingestion_pipeline.py
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }} # Necessário para o script Python

      # PASSO 2: EXECUÇÃO DO DBT (T & Testes)
      - name: Run dbt (Transformations and Tests)
        run: |
          # 1. O profiles.yml e o dbt_project.yml precisam do PROJECT_ID
          # 2. Usamos a flag --profile e o caminho absoluto para profiles.yml
          dbt run --profiles-dir $GITHUB_WORKSPACE --project-dir $DBT_PROJECT_DIR --profile atlas_capital_data_pipeline
          dbt test --profiles-dir $GITHUB_WORKSPACE --project-dir $DBT_PROJECT_DIR --profile atlas_capital_data_pipeline
        env:
          GCP_PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}