# üìä **Atlas Capital: Pipeline ELT e Modelagem de Indicadores Macroecon√¥micos**

---

## üí° **Simula√ß√£o de um cen√°rio de neg√≥cio**

A **Atlas Capital (empresa fict√≠cia)** √© um fundo de investimentos de pequeno porte, especializado em mercados emergentes. Seu time de an√°lise acompanha indicadores macroecon√¥micos ‚Äî como PIB, infla√ß√£o e desemprego ‚Äî para embasar decis√µes estrat√©gicas em pa√≠ses-chave da Am√©rica Latina, √Åsia e √Åfrica.

Antes deste projeto, os dados eram coletados manualmente a partir de fontes p√∫blicas, gerando retrabalho, inconsist√™ncias e alto custo operacional. Com or√ßamento zero para aquisi√ß√£o de solu√ß√µes comerciais, surgiu a necessidade de construir um sistema automatizado de fornecimento de dados.

Utilizando apenas ferramentas <u>open source e dados p√∫blicos confi√°veis</u>, este projeto implementa um pipeline de dados totalmente automatizado, consumindo dados da World Bank Data360, padronizando indicadores macroecon√¥micos e disponibilizando-os em um Data Warehouse anal√≠tico no BigQuery, pronto para an√°lises, visualiza√ß√µes e expans√£o futura.

---

## üèóÔ∏è **Arquitetura do Projeto (Modern Data Stack)**

O projeto segue o paradigma ELT (Extract, Load, Transform), priorizando o processamento dentro do Data Warehouse para escalabilidade e baixo custo.

![Lineage graph](image.png)

---

### ‚öôÔ∏è **Stack Tecnol√≥gica**

| Componente     | Tecnologia      | Papel no Pipeline                                                  |
|:---------------|:----------------|:-------------------------------------------------------------------|
| Linguagem      | Python 3.12     | Scripts de extra√ß√£o de API e carga resiliente.                     |
| Data Warehouse | Google BigQuery | Armazenamento colunar em nuvem (Camadas Raw, Staging e Marts).     |
| Transforma√ß√£o  | dbt Core        | Engenharia Anal√≠tica, modelagem dimensional e testes de qualidade. |
| Orquestra√ß√£o   | GitHub Actions  | Automa√ß√£o di√°ria (Cron Job) e integra√ß√£o cont√≠nua (CI/CD).         |
| Infraestrutura | Terraform       | Defini√ß√£o da arquitetura de datasets como c√≥digo (IaC).            |
| Visualiza√ß√£o   | Looker Studio   | Dashboard executivo para tomada de decis√£o.                        |

---

## üöÄ **Detalhes T√©cnicos e Engenharia**

1. **Ingest√£o (Python)**
* Extra√ß√£o de 5 indicadores macroecon√¥micos para 6 pa√≠ses emergentes.
* **Resili√™ncia:** Tratamento de erros de API e loops iterativos para contornar limita√ß√µes de endpoints p√∫blicos.
* **Idempot√™ncia:** Cargas configuradas para garantir que execu√ß√µes repetidas n√£o dupliquem dados.
2. **Modelagem de Dados (dbt Core)**
* **Arquitetura de Medalh√£o:** Organiza√ß√£o em camadas stg_ (limpeza), dim_ (dimens√µes) e fact_ (fatos).
* **Star Schema:** Implementa√ß√£o de tabelas dimensionais (dim_pais, dim_tempo) e tabela fato pivoteada para alta performance em BI.
* **Data Quality:** 10/10 testes aprovados (Unique, Not Null e Referential Integrity).
3. **Automa√ß√£o (CI/CD)**
* Pipeline configurado no GitHub Actions para rodar diariamente sem interven√ß√£o manual.
* Inje√ß√£o din√¢mica de segredos (GCP Service Account) via Base64 para m√°xima seguran√ßa.

---

## ‚úÖ **Resultados**

* **Situa√ß√£o:** Decis√µes de investimento baseadas em processos manuais e dados n√£o validados.
* **Tarefa:** Automatizar o fluxo fim a fim com custo zero de infraestrutura.
* **A√ß√£o:** Implementei um pipeline ELT completo integrando Python, BigQuery e dbt com automa√ß√£o via GitHub Actions.
* **Resultado:** Redu√ß√£o de 100% no esfor√ßo manual de coleta e 100% de confian√ßa nos dados atrav√©s de testes automatizados, al√©m de <u>custo zero de infraestrutura</u>.

---

## üîó **Links e Artefatos**

* **Dashboard Interativo:** https://lookerstudio.google.com/reporting/b6ce62da-b856-4e12-a64c-80e30c5c75a8
* **Documenta√ß√£o dbt (Lineage):** https://revsonv.github.io/atlas-capital-data-pipeline/#!/overview
* **Perfil no LinkedIn:** https://www.linkedin.com/in/revson-vieira-0a753057/

---

## üõ†Ô∏è **Como Reproduzir**

1. Clone o reposit√≥rio.
2. Configure as credenciais do GCP no arquivo .env (local) ou GitHub Secrets.
3. Execute pip install -r requirements.txt.
4. Rode o script de ingest√£o: python src/ingestion_pipeline.py.
5. Execute as transforma√ß√µes: dbt run e dbt test.

*Desenvolvido por Revson Vieira como projeto de portf√≥lio para Engenharia de Dados.*